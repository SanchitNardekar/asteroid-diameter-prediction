{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sklearn\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LinearRegression, Lasso, ElasticNet\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n\nfrom scipy.stats import norm, t\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport statsmodels.api as sm","metadata":{"ExecuteTime":{"end_time":"2020-03-05T11:57:47.240075Z","start_time":"2020-03-05T11:57:45.030632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-processing","metadata":{}},{"cell_type":"markdown","source":"## Import data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/prediction-of-asteroid-diameter/Asteroid_Updated.csv\", \n                 low_memory=False)","metadata":{"ExecuteTime":{"end_time":"2020-03-05T22:14:57.838684Z","start_time":"2020-03-05T22:14:53.639995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First look at the first 5 rows of data. Useful to get an initial feel for the dataset.","metadata":{}},{"cell_type":"code","source":"pd.set_option('max_columns', 31)\ndf.head(5)","metadata":{"ExecuteTime":{"end_time":"2020-03-05T22:14:58.420045Z","start_time":"2020-03-05T22:14:58.392761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check the shape and column names of the dataset.","metadata":{}},{"cell_type":"code","source":"df.shape, df.columns","metadata":{"ExecuteTime":{"end_time":"2020-03-05T22:14:58.872116Z","start_time":"2020-03-05T22:14:58.8674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Look into the data types of each variable","metadata":{}},{"cell_type":"code","source":"pd.DataFrame({\"Column\":df.columns, \n              \"DType\": df.dtypes.values, \n              \"NonNulls\": [df[col].count() for col in df.columns]})","metadata":{"ExecuteTime":{"end_time":"2020-03-05T22:14:59.796491Z","start_time":"2020-03-05T22:14:59.371572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning","metadata":{}},{"cell_type":"markdown","source":"From above we can see our target feature, diameter, should be numeric but instead pandas has loaded it as an object dtype. \n\nIt is likely there are some non-numerical values in the diameter column so we need to identify whether they can be converted and if not, either manually convert or remove them from the dataset.","metadata":{}},{"cell_type":"code","source":"try: \n    df.diameter = pd.to_numeric(df.diameter)\nexcept ValueError as e:\n    print(\"ERROR:\", e)","metadata":{"ExecuteTime":{"end_time":"2020-03-05T22:15:00.39535Z","start_time":"2020-03-05T22:15:00.382112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"converted_vals = []\nfor idx, val in enumerate(df.diameter.tolist()):\n    if isinstance(val, str):\n        converted_vals.append(float(val))\n    else:\n        converted_vals.append(float(val))\ndf.diameter = converted_vals\ndf.diameter = pd.to_numeric(df.diameter)\ndf.diameter.dtype","metadata":{"ExecuteTime":{"end_time":"2020-03-05T22:15:02.023854Z","start_time":"2020-03-05T22:15:01.028873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop features that have a proportion of null values greater than 5%.","metadata":{}},{"cell_type":"code","source":"df_cleaned = df[df.diameter.notnull()]\ndf_cleaned = df_cleaned[[col for col in df_cleaned.columns if (df_cleaned[col].count() / len(df_cleaned)) > 0.95]]\ndf_cleaned.shape, df_cleaned.columns","metadata":{"ExecuteTime":{"end_time":"2020-03-05T22:15:02.963233Z","start_time":"2020-03-05T22:15:02.747574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_numerical = df_cleaned[[col for col, dtype in zip(df_cleaned.columns, df_cleaned.dtypes) if dtype != object]]\ndf_numerical.shape, df_numerical.columns","metadata":{"ExecuteTime":{"end_time":"2020-03-05T22:15:03.779227Z","start_time":"2020-03-05T22:15:03.753406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop rows that have values below 0","metadata":{}},{"cell_type":"code","source":"def drop_negative(df):\n    for col in df:\n        if df[col].dtype == 'object':\n            continue\n        else:\n            if df[col][df[col] < 0].count() > 0:\n                df.drop(df[df[col] < 0].index, axis=0, inplace=True)\n            else:\n                continue\ndrop_negative(df_numerical)","metadata":{"ExecuteTime":{"end_time":"2020-03-05T22:15:04.582734Z","start_time":"2020-03-05T22:15:04.555077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop rows with one null","metadata":{}},{"cell_type":"code","source":"df_numerical = df_numerical.dropna()\ndf_numerical.shape","metadata":{"ExecuteTime":{"end_time":"2020-03-05T22:15:05.355965Z","start_time":"2020-03-05T22:15:05.293987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_numerical.describe()","metadata":{"ExecuteTime":{"end_time":"2020-03-05T22:15:06.421316Z","start_time":"2020-03-05T22:15:06.139502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation Heatmap","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16, 10))\nax = sns.heatmap(df_numerical[1:].corr(method='pearson'), \n            annot=True, \n            cmap=sns.color_palette('magma'),\n            linewidth=0.3, \n            edgecolor='k')\nbot, top = ax.get_ylim()        # To solve bug where sns.heatmap top and bottom axes\nax.set_ylim(bot + 0.5, top-0.5) # are cutoff in Matplotlib 3.1.1 (current version)\nplt.title('Correlation Heatmap')\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2020-03-05T22:15:11.001326Z","start_time":"2020-03-05T22:15:09.028582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"def VIF_calc(df):\n    aux_df = df.assign(const=1)\n    vif = pd.Series([variance_inflation_factor(aux_df.values, i) \n               for i in range(len(aux_df.columns))], \n              index=aux_df.columns)\n    return vif\n\nVIF_calc(df_numerical)","metadata":{"ExecuteTime":{"end_time":"2020-03-05T22:15:15.907436Z","start_time":"2020-03-05T22:15:11.740842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_numerical = df_numerical[[col for col, vif in zip(df_numerical.columns, VIF_calc(df_numerical).values) if vif < 5]]","metadata":{"ExecuteTime":{"end_time":"2020-03-05T22:15:24.306002Z","start_time":"2020-03-05T22:15:20.17867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Modeling","metadata":{}},{"cell_type":"markdown","source":"## Linear Regression (Base Model)","metadata":{}},{"cell_type":"code","source":"def standard_error(df, col, rss_calc):\n    x_bar = np.mean(df[col].tolist())\n    feature_std = np.sqrt(sum([(xi - x_bar)**2 for xi in df[col].tolist()]))\n    rse = np.sqrt(rss_calc / (len(df) - 2))\n    return rse / feature_std","metadata":{"ExecuteTime":{"end_time":"2020-03-05T22:15:27.679611Z","start_time":"2020-03-05T22:15:27.672263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def linear_model_eval(X, Y):\n    \n    reg = LinearRegression(normalize=False)\n\n    kfold = KFold(n_splits = 10, shuffle=True, random_state = 42)\n\n    scores = []\n    rmses = []\n    for train_index, test_index in kfold.split(X):\n        x_train, x_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], Y.iloc[train_index], Y.iloc[test_index]\n\n        reg.fit(x_train, y_train)\n        scores.append([reg.score(x_test, y_test),\n                       (np.sqrt(mean_squared_error(y_test, reg.predict(x_test))))])\n\n    print(\"K-Fold CV:\\n\", pd.DataFrame(data = np.array(scores), columns = ['RSquared', 'RMSE']))\n\n    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state = 999, shuffle = True)\n\n    reg.fit(x_train, y_train)\n    \n    df_coef = pd.DataFrame(data = np.array([[m, n] for m, n in zip(X.columns,reg.coef_)]), columns = ['column', 'coef'])\n    rss_calc = mean_squared_error(y_test, reg.predict(x_test)) * len(X)\n    df_coef['SE'] = [standard_error(X, col, rss_calc) for col in X.columns]\n    df_coef.coef = pd.to_numeric(df_coef.coef)\n    df_coef['t'] = df_coef['coef'] / df_coef['SE']\n    df_coef['pvalue'] = df_coef.apply(lambda row: t.sf(np.abs(row['t']), len(X)-1)*100 ,axis=1)\n    print(\"Coefficient Analysis:\\n\", df_coef)\n    \n    print(\"R-Squared: {0:.3f} \\nRMSE: {1:.3f}\".format(reg.score(x_test, y_test), \n                                    np.sqrt(mean_squared_error(y_test, reg.predict(x_test)))))\n    #print(reg.intercept_, np.sqrt(rss_calc*len(X)) * np.sqrt(1./len(X)) + )\n    sns.distplot(y_test.values, kde=True, label = 'True')\n    sns.distplot(reg.predict(x_test), kde=True, label = 'Prediction')\n    plt.legend()\n    plt.show()\n    return reg\n\nX = df_numerical.drop('diameter', axis=1)\nY = df_numerical.diameter\n\nlinear_model_eval(X,Y)","metadata":{"ExecuteTime":{"end_time":"2020-03-05T22:15:31.698341Z","start_time":"2020-03-05T22:15:28.360772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above analysis we can see that only **i**, **data_arc**, **albdeo**, and **n** have the greatest effect on the output of the model, statistically speaking. Therefore, removing all other variables should give us a very similar R-Squared.","metadata":{}},{"cell_type":"code","source":"X = df_numerical[['i', 'data_arc', 'albedo', 'n']]\nY = df_numerical.diameter\n\nlinear_model_eval(X,Y)","metadata":{"ExecuteTime":{"end_time":"2020-03-05T22:19:54.095804Z","start_time":"2020-03-05T22:19:52.309175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VIF_calc(X)","metadata":{"ExecuteTime":{"end_time":"2020-03-05T15:09:21.731128Z","start_time":"2020-03-05T15:09:21.592167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"R-Squared is exactly the same with RMSE being very similar with just these 4 features, however it doesn't explain the variance very well as eluded to by a low R-Squared. Therefore, this first pass model doesn't include the correct features to properly explain the variance seen in the diameter.","metadata":{}},{"cell_type":"markdown","source":"## Reconstitution of features","metadata":{}},{"cell_type":"markdown","source":"Now that we have a base model, let's do some feature engineering to include features that we arbitrarily removed due to high VIF and try to glean a better fit. ","metadata":{}},{"cell_type":"code","source":"print(\"Before:\", df_cleaned.shape)\ndf_numerical = df_cleaned[[col for col, dtype in zip(df_cleaned.columns, df_cleaned.dtypes) if dtype != object]]\ndrop_negative(df_numerical)\ndf_numerical = df_numerical.dropna()\nprint(\"After:\", df_numerical.shape)","metadata":{"ExecuteTime":{"end_time":"2020-03-05T15:12:50.030879Z","start_time":"2020-03-05T15:12:49.979874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using the correlation heatmap, VIF scores, and the descriptions of the features we can drop:\n \n* **per_y** - Orbital period in years : as this is given in days in **per**\n* **moid** - Minimum Orbit Intersection Distance : as this is strongly correlated with the perihelion, **q**.","metadata":{}},{"cell_type":"code","source":"df_numerical = df_numerical.drop(['per_y', 'moid'], axis=1)","metadata":{"ExecuteTime":{"end_time":"2020-03-05T15:12:50.631119Z","start_time":"2020-03-05T15:12:50.617121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear model","metadata":{}},{"cell_type":"code","source":"X = df_numerical.drop('diameter', axis=1)\nY = df_numerical.diameter\n\nlinear_model_eval(X,Y)","metadata":{"ExecuteTime":{"end_time":"2020-03-05T15:13:30.492768Z","start_time":"2020-03-05T15:13:28.368765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VIF_calc(X)","metadata":{"ExecuteTime":{"end_time":"2020-03-05T15:14:17.820575Z","start_time":"2020-03-05T15:14:16.023618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Higher R-Squared achieved! Although we see a couple of features are insignificant and a few features have infinite VIF. Therefore these ought to be removed.","metadata":{}},{"cell_type":"markdown","source":"### Linear model -  $ln(diameter)$","metadata":{}},{"cell_type":"markdown","source":"Transform each column (features and target) by evaluating the natural log of each data point. Thanks to Liam Toran in his [notebook](https://www.kaggle.com/liamkesatoran/asteroid-diameter-estimators-with-added-difficulty) for suggesting a natural log transformation of diameter. \n\nThus this generates a more robust, accurate, and precise linear model, as evidenced by the low standard errors and p-values below.","metadata":{}},{"cell_type":"code","source":"X = df_numerical.drop(['diameter'], axis=1)\nfor col in X.columns:\n    X[\"log \"+ col] = X[col].apply(np.log)\n    X = X.drop(col, axis=1)\n#X = pd.DataFrame(StandardScaler().fit_transform(X), columns = X.columns)\nY = np.log(df_numerical.diameter)\n\nlinear_model_eval(X,Y)","metadata":{"ExecuteTime":{"end_time":"2020-03-05T15:17:44.593264Z","start_time":"2020-03-05T15:17:42.271261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The high R-Squared is suspicious however, we ought to check further for multicollinearity between features.","metadata":{}},{"cell_type":"code","source":"VIF_calc(X)","metadata":{"ExecuteTime":{"end_time":"2020-03-03T10:26:59.753198Z","start_time":"2020-03-03T10:26:58.159117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As evidenced above, a number of features have a VIF > 5 with a few features that are directly proportional to other features.","metadata":{}},{"cell_type":"markdown","source":"Therefore, remove **per**, the Orbital Period in days, as this is an input into calculating **n**, the Mean motion, a parameter in Orbital Mechanics evaluated through the following equation:\n\n$\n\\begin{equation}\n\\Large\nn = \\frac{2 \\pi}{P}\n\\end{equation}\n$\n\nAlso drop **a**, the semi-major axis, as it interacts with **q**, the perihelion distance, where **a** is the average between the perihelion, **q**, and aphelion, **ad**.\n\n$\n\\large\nad = a(1-e)\n\\\\\n\\large\nq = a(1+e)\n\\\\\n\\large\n\\therefore a = \\frac{q + ad}{2}\n$","metadata":{}},{"cell_type":"code","source":"X = df_numerical.drop(['diameter', 'per', 'a', 'H', 'albedo'], axis=1)\nfor col in X.columns:\n    X[\"log \"+ col] = X[col].apply(np.log)\n    X = X.drop(col, axis=1)\n#X = pd.DataFrame(StandardScaler().fit_transform(X), columns = X.columns)\nY = np.log(df_numerical.diameter)\n\nlinear_model_eval(X,Y)","metadata":{"ExecuteTime":{"end_time":"2020-03-03T10:27:01.518508Z","start_time":"2020-03-03T10:26:59.754201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VIF_calc(X)","metadata":{"ExecuteTime":{"end_time":"2020-03-03T10:27:02.248463Z","start_time":"2020-03-03T10:27:01.519513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is still a high correlation between **ln(q)**, **ln(ad)** and **ln(n)**.\n\nFrom knowledge of [Kepler's 3rd Law](https://en.wikipedia.org/wiki/Mean_motion#Mean_motion_and_Kepler's_laws), we understand that the semi-major axis (**a**) and mean motion (**n**) are proportional to one another through the relation:\n\n$\n\\mu = a^3 n^2\n$\n\nwhere $\\mu$ is the standard gravitational parameter in au/days for a heliocentric 2-body system.\n\nTherefore, we can drop **q** and **ad** as they make up **a** and so are proportional to the Mean Motion **n**.","metadata":{}},{"cell_type":"markdown","source":"### Best non-log linear fit (standardised - no vif change)","metadata":{}},{"cell_type":"code","source":"X = df_numerical.drop(['diameter', 'a', 'q', 'ad', 'per'], axis=1)\n#X['n_obs_used:data_arc'] = X.n_obs_used * X.data_arc\n#X['e:n'] = X.e * X.n\n# X['H:n_obs_used'] = X.H * X.n_obs_used\nX = pd.DataFrame(StandardScaler().fit_transform(X), columns = X.columns)\nY = np.log(df_numerical.diameter)\n\nlinear_model_eval(X,Y)","metadata":{"ExecuteTime":{"end_time":"2020-03-05T15:31:23.928555Z","start_time":"2020-03-05T15:31:22.43855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VIF_calc(X)","metadata":{"ExecuteTime":{"end_time":"2020-03-03T10:27:04.293134Z","start_time":"2020-03-03T10:27:03.699364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Best log linear fit","metadata":{}},{"cell_type":"code","source":"X = df_numerical.drop(['diameter', 'a', 'q', 'ad', 'per'], axis=1)\nX['e:n'] = X.e * X.n\nfor col in X.columns:\n    X[\"log \"+ col] = X[col].apply(np.log)\n#     X = X.drop(col, axis=1)\nX['log e:log n'] = X['log e'] * X['log n']\n#X['log H: log n_obs_used'] = X['log H'] * X['log n_obs_used']\nX = X.drop('log e:n', axis=1)\nX = pd.DataFrame(StandardScaler().fit_transform(X), columns = X.columns)\nY = np.log(df_numerical.diameter)\n\nlinear_model_eval(X,Y)","metadata":{"ExecuteTime":{"end_time":"2020-03-05T15:29:01.682763Z","start_time":"2020-03-05T15:28:58.481755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VIF_calc(X)","metadata":{"ExecuteTime":{"end_time":"2020-03-05T15:29:30.667631Z","start_time":"2020-03-05T15:29:26.04863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And just to prove that using **n** alone is better than using linear combinations of **q**, **ad** and **a**:","metadata":{}},{"cell_type":"code","source":"X = df_numerical.drop(['diameter', 'n', 'per', 'albedo', 'H'], axis=1)\nY = np.log(df_numerical.diameter)\n\nlinear_model_eval(X,Y)","metadata":{"ExecuteTime":{"end_time":"2020-03-03T10:27:12.099346Z","start_time":"2020-03-03T10:27:10.26995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VIF_calc(X)","metadata":{"ExecuteTime":{"end_time":"2020-03-03T10:27:12.966476Z","start_time":"2020-03-03T10:27:12.100347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_logged = df_numerical.copy()\n# for col in df_logged.columns:\n#     df_logged[\"ln \"+ col] = df_logged[col].apply(np.log)\n#     df_logged = df_logged.drop(col, axis=1)  \ndf_logged['ln diameter'] = np.log(df_logged.diameter)\nfor col in df_numerical.columns:\n    plt.figure(figsize=(15, 6))\n    plt.subplot(1, 2, 1)\n    sns.scatterplot(x=col, y='diameter', data=df_numerical)\n    plt.title(\"Raw\", fontsize=14)\n    plt.subplot(1, 2, 2)\n    sns.scatterplot(x=col, y='ln diameter', data=df_logged)\n    plt.title(\"Logged\", fontsize=14)\n    plt.show()","metadata":{"ExecuteTime":{"end_time":"2020-03-03T10:27:38.814633Z","start_time":"2020-03-03T10:27:12.96848Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gradient Boosting","metadata":{}},{"cell_type":"markdown","source":"### Base Model","metadata":{}},{"cell_type":"code","source":"def ensemble_model_eval(X, Y):\n    \n    reg = GradientBoostingRegressor(\n    loss='ls',\n    learning_rate=0.1,\n    n_estimators=1000,\n    subsample=1.0,\n    criterion='friedman_mse',\n    min_samples_split=20,\n    min_samples_leaf=20,\n    min_weight_fraction_leaf=0.0,\n    max_depth=10,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    init=None,\n    random_state=42,\n    max_features=None,\n    alpha=0.9,\n    verbose=0,\n    max_leaf_nodes=50,\n    warm_start=False,\n    presort='auto',\n    validation_fraction=0.1,\n    n_iter_no_change=None,\n    tol=0.0001)\n\n#     kfold = KFold(n_splits = 10, shuffle=True, random_state = 42)\n\n#     scores = []\n#     for train_index, test_index in kfold.split(X):\n#         x_train, x_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], Y.iloc[train_index], Y.iloc[test_index]\n\n#         reg.fit(x_train, y_train)\n#         scores.append([reg.score(x_test, y_test),\n#                        (np.sqrt(mean_squared_error(y_test, reg.predict(x_test))))])\n\n#     print(\"K-Fold CV:\\n\", pd.DataFrame(data = np.array(scores), columns = ['RSquared', 'RMSE']))\n\n    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state = 42, shuffle = True)\n\n    reg.fit(x_train, y_train)\n    \n    print(\"R-Squared: {0:.3f} \\nRMSE: {1:.3f}\".format(reg.score(x_test, y_test), \n                                    np.sqrt(mean_squared_error(y_test, reg.predict(x_test)))))\n    \n    sns.distplot(y_test.values, kde=True, label = 'True')\n    sns.distplot(reg.predict(x_test), kde=True, label = 'Prediction')\n    plt.legend()\n    plt.show()\n    return reg\n\nX = df_numerical.drop(['diameter', 'albedo', 'H'], axis=1)\nX['e:n'] = X.e * X.n\nY = np.log(df_numerical.diameter)\n\nreg = ensemble_model_eval(X,Y)","metadata":{"ExecuteTime":{"end_time":"2020-03-03T10:32:55.707464Z","start_time":"2020-03-03T10:27:38.815596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_importance = pd.DataFrame(reg.feature_importances_, index = X.columns, columns=['Importance'])\ndf_importance.sort_values(by=['Importance'],  axis=0, ascending=False)","metadata":{"ExecuteTime":{"end_time":"2020-03-03T10:32:55.724466Z","start_time":"2020-03-03T10:32:55.708466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_for_duplicates(df):\n    return len(df.groupby(df.columns.tolist(), axis=0).size().reset_index().rename(columns={0:'count'})) == len(df)\n\ncheck_for_duplicates(df_numerical)","metadata":{"ExecuteTime":{"end_time":"2020-03-03T10:32:56.264467Z","start_time":"2020-03-03T10:32:55.725466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(X.n, reg.predict(X), label='prediction')\nplt.scatter(X.n, Y, label='true')\nplt.legend()\nplt.subplot(1, 2, 2)\nplt.scatter(X.n, reg.predict(X) - Y, label='residuals')\nplt.legend()\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2020-03-03T10:33:04.117465Z","start_time":"2020-03-03T10:32:56.265468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(reg.predict(X)-Y, bins=np.linspace(-3, 3, 201))\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2020-03-03T10:33:06.313469Z","start_time":"2020-03-03T10:33:04.118468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}